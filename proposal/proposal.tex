\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{381V Visual Recognition Final Project Proposal}

\author{Keivaun Waugh\\
University of Texas at Austin\\
{\tt\small keivaunwaugh@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Paul Choi\\
University of Texas at Austin\\
{\tt\small choipaul96@gmail.com}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    In this project, we plan to explore the benefits of providing semantic segmentation data to an image super-resolution (SR) system. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Image super-resolution is a compelling area of computer vision. Being able to
convert low resolution photos and video to higher resolution counterparts is an
important problem as hardware continues to improve. With higher resolution
displays and cameras available today than ever before, it would be nice if we
could convert old media to the same resolution standards as current photos and
video. Naive methods of upsampling using bicubic interpolation introduce many
artifacts and produce visually displeasing images. With the increase in
popularity of deep networks sparked by work from Krizhevsky et al.
\cite{AlexNet}, it appears that some variant of a deep network could provide
good results for this task. We also plan to combine semantic segmentation with
state-of-the-art SR techniques to provide for better results.

%-------------------------------------------------------------------------

\section{Related Work}
Prior work involving image synthesis with deep networks involves one of two
techniques: using a more conventional CNN for upsampling
\cite{PerceptualLosses} \cite{RealtimeCNN} \cite{DeeplyRecursive} or using a
generative adversarial network (GAN) \cite{GAN}. Typically, the CNN approaches
attempt to minimize the per-pixel loss between the upsampled output image and
the original unmodified image. The error metric frequently used is peak
signal-to-noise ration (PSNR). However, when this is applied directly on the
pixel space, this often encourages the network to make soft changes in the
upsampled image rather than generate the high-frequency changes often found in
real images. GAN based approaches like in \cite{SRGAN} attempt to overcome this
by using an adversarial network. Though they get lower PSNR values, the images
often look more realistic, which suggests that some other metric should be used
to get better results.

In \cite{ImageSynthesis}, Chen and Koltun experiment with using semantic
segmentation from the cityscapes dataset \cite{Cityscapes} to perform image
generation of arbitrary size. However, to our knowledge, there have been no
papers published that attempt to merge together this explicit semantic data
with a SR technique for better results. We believe that this will provide for
better results than what is currently achievable with the published methods.

%-------------------------------------------------------------------------

\section{Technical Plan}

%-------------------------------------------------------------------------

\section{Experimental Plan}

%-------------------------------------------------------------------------

\section{Sources of Data}
Image SR techniques are self-supervised in that explicit annotation of data is unneeded. The original image can just be downsized before it is fed into the SR network. The SR output can then be compared to the original image. However, in order to integrate semantic segmentation knowledge, we need to obtain object masks from some location. There are two ways that we plan on going about this. The first is to use datasets that already include these masks such as the cityscape dataset. The other approach is to use existing segmentation networks like in \cite{FullyConvolutionalSS} to generate the mask before the downsized image is fed into our SR network. We plan to compare the results of the two to see is the more accurate human annotated segmentations are required to get good results from our technique.

%-------------------------------------------------------------------------

\section{Partner Plan}

%-------------------------------------------------------------------------

\section{Speculation of Results}
Current state-of-the-art semantic segmentation methods already involve CNNs.
One may argue that a CNN based approach for SR would automatically learn
information regarding segmentation if it is useful for providing better
results. However, this argument does not take into account the difficulty of
learning such information. A parallel argument was made for residual networks
in \cite{ResNet} with regards to learning identity mappings. However, the
authors found that it was often difficult for networks to learn such mappings.
We predict that adding the semantic segmentation data explicitly to the network
will allow it to make more intelligent decisions along object boundaries. We
expect this will help provide more crisp edges. Because of the success with
using semantic masks alongside CNNs for image generation of arbitrary size in
\cite{ImageSynthesis}, we expect that our results will be better at high
upsampling factors.

{\small
\bibliographystyle{ieee}
\bibliography{bib}
}

\end{document}
