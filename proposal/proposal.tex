\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{CS 381V Visual Recognition Final Project Proposal}

\author{Keivaun Waugh\\
University of Texas at Austin\\
{\tt\small keivaunwaugh@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Paul Choi\\
University of Texas at Austin\\
{\tt\small choipaul96@gmail.com}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this project, we plan to explore the benefits of providing semantic
segmentation data to an image super-resolution (SR) system.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Image super-resolution is a compelling area of computer vision. Being able to
convert low resolution photos and video to higher resolution counterparts is an
important problem as hardware continues to improve. With higher resolution
displays and cameras more available today than ever before, it would be nice if
we could convert old media to the same resolution standards as current photos
and video. Naive methods of upsampling using bicubic interpolation introduce
many artifacts and produce visually displeasing images. Deep networks have
greatly increased in popularity for instance and category recognition, sparked
by work from Krizhevsky et al. \cite{AlexNet}. SR methods based on deep networks
have also achieved considerable success in SR tasks. We plan to combine
semantic segmentation features with state-of-the-art SR techniques in order to
achieve sharper SR solutions.

%------------------------------------------------------------------------------

\section{Related Work}
Prior work involving image synthesis with deep networks involves one of two
techniques: using a more conventional CNN for upsampling
\cite{PerceptualLosses} \cite{RealtimeCNN} \cite{DeeplyRecursive} or using a
generative adversarial network (GAN) \cite{GAN}. Typically, the CNN approaches
attempt to minimize the per-pixel loss between the upsampled output image and
the original unmodified image. The error metric frequently used is peak
signal-to-noise ratio (PSNR). However, when this is applied directly on the
pixel space, this often encourages the network to make soft changes in the
upsampled image rather than generate the high-frequency changes often found in
real images. GAN based approaches like the work of Ledig et al. \cite{SRGAN}
attempt to overcome this by using an adversarial network. Though they get lower
PSNR values, the images often look more realistic, which suggests that some
other metric should be optimized to get better results.

In \cite{ImageSynthesis}, Chen and Koltun experiment with using semantic
segmentation from the Cityscapes dataset \cite{Cityscapes} to perform
photorealistic image synthesis. However, to our knowledge, there have been no
papers published that attempt to merge together this explicit semantic data
with a SR technique for better results. We believe that this will provide for
better results than what is currently achievable with the published methods.

%------------------------------------------------------------------------------

\section{Technical Plan}
The authors of \cite{SRGAN} have their code for their SRGAN network available
online. We will use this code as a starting point and modify it to include our
semantic segmentation data from the datasets or out-of-the-box segmentation
networks. One method we will use for integration this additional data will be
to concatenate the segmentation data onto the feature maps of the early layers
in the network. This will be evaluated on both the traditional ResNet variant
that the authors introduce as well as the GAN version.  

%------------------------------------------------------------------------------

\section{Experimental Plan}
We will build upon the open-source implementation of Ledig et al. \cite{SRGAN}
which uses TensorFlow 1.2. Our first task will be to apply the unmodified
network to a dataset of our choosing which includes semantic segmentation
labels. These results will constitute our baseline. We will use the same
measures to evaluate our SR method: PSNR, structural similarity (SSIM), and
mean opinion score (MOS). For MOS, we will conduct a survey as in \cite{SRGAN}
and ask raters to rate images produced by different methods on a scale of 1 to
5 in terms of quality. While the original paper uses 26 raters, we will likely
use fewer raters to ease the logistical burden. As an additional baseline, we
will also measure the performances of the nearest neighbor and bicubic
interpolation methods.

Our second and main task will be to modify the SRResNet and SRGAN networks to
utilize semantic segmentation features, as discussed in the technical plan. If
we have enough time, we will also experiment with adding these additional
features to the discriminator in the SRGAN network to see if it aids training.
The measurements from these modified approaches will indicate the effectiveness
of our method.

In addition to using a dataset with semantic segmentation labels, we will also
use datasets without such labels included and generate the labels manually
using an out-of-the-box solution. This will allow us to compare how
human-annotated segmentation features compare to machine-generated segmentation
features.

We will experiment with adding the segmentation data at multiple places in the
network. The most basic solution would be to add it onto the feature maps at
the earliest layers in the network. However, we plan to evaluate the network
with the segmentations included at different locations and multiple locations.

%------------------------------------------------------------------------------

\section{Sources of Data}
Image SR techniques are self-supervised in that explicit annotation of data is
unneeded. Given an image, a training pair can easily be generated by using the
downsampled image as the input to the SR network and the original image as the
ground truth. However, in order to integrate semantic segmentation knowledge,
we need to obtain the semantic layout for each image. There are two ways that we
plan on going about this. The first is to use datasets that already include
the segmentation such as the Cityscapes \cite{Cityscapes} dataset. The other
approach is to use existing segmentation networks as in
\cite{FullyConvolutionalSS} to generate the segmentation before the downsized
image is fed into our SR network. We plan to compare the results of the two to
see if the more accurate human-annotated segmentations are required to get good
results from our technique, or if a fully-automated SR approach can be achieved.

To compare our results against those obtained from other papers, we plan on
using standard evaluation datasets that are used in \cite{SRGAN}. These include
Set5 \cite{Set5}, Set14 \cite{Set14}, and BSD100 which is a subset of
BSD300 \cite{BSD300}

%------------------------------------------------------------------------------

\section{Partner Plan}
For the programming portion of the project, we plan to pair-program and
divide equally the implementation work. For experiments, we will individually
drive each one while working together as needed, with each person driving an
equal number of experiments. The report will be written collaboratively. The
person with more expertise on a given section will lead writing for that
section.

%------------------------------------------------------------------------------

\section{Speculation of Results}
Current state-of-the-art semantic segmentation methods already involve CNNs.
One may argue that a CNN based approach for SR would automatically learn
information regarding segmentation if it is useful for providing better
results. However, this argument does not take into account the difficulty of
learning such information. A parallel argument was made for residual networks
in \cite{ResNet} with regards to learning identity mappings. However, the
authors found that it was often difficult for networks to learn such mappings.
We predict that adding the semantic segmentation data explicitly to the network
will allow it to make more intelligent decisions along object boundaries. We
expect this will help provide crisper edges. Because of the success with using
semantic masks at various resolutions in \cite{ImageSynthesis}, we expect that
our results will be better at high upsampling factors.

{\small
\bibliographystyle{ieee}
\bibliography{bib}
}

\end{document}
