\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{CS 381V Visual Recognition Final Project Proposal}

\author{Keivaun Waugh\\
University of Texas at Austin\\
{\tt\small keivaunwaugh@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Paul Choi\\
University of Texas at Austin\\
{\tt\small choipaul96@gmail.com}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this project, we plan to explore the benefits of providing semantic
segmentation data to an image super-resolution (SR) system.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Image super-resolution is a compelling area of computer vision. Being able to
convert low resolution photos and video to higher resolution counterparts is an
important problem as hardware continues to improve. With higher resolution
displays and cameras more available today than ever before, it would be nice if
we could convert old media to the same resolution standards as current photos
and video. Naive methods of upsampling using bicubic interpolation introduce
many artifacts and produce visually displeasing images. With the increase in
popularity of deep networks sparked by work from Krizhevsky et al.
\cite{AlexNet}, it appears that some variant of a deep network could provide
good results for this task. We also plan to combine semantic segmentation with
state-of-the-art SR techniques to provide for better results.

%-------------------------------------------------------------------------

\section{Related Work}
Prior work involving image synthesis with deep networks involves one of two
techniques: using a more conventional CNN for upsampling
\cite{PerceptualLosses} \cite{RealtimeCNN} \cite{DeeplyRecursive} or using a
generative adversarial network (GAN) \cite{GAN}. Typically, the CNN approaches
attempt to minimize the per-pixel loss between the upsampled output image and
the original unmodified image. The error metric frequently used is peak
signal-to-noise ration (PSNR). However, when this is applied directly on the
pixel space, this often encourages the network to make soft changes in the
upsampled image rather than generate the high-frequency changes often found in
real images. GAN based approaches like in \cite{SRGAN} attempt to overcome this
by using an adversarial network. Though they get lower PSNR values, the images
often look more realistic, which suggests that some other metric should be used
to get better results.

In \cite{ImageSynthesis}, Chen and Koltun experiment with using semantic
segmentation from the Cityscapes dataset \cite{Cityscapes} to perform
photorealistic image synthesis. However, to our knowledge, there have been no
papers published that attempt to merge together this explicit semantic data
with a SR technique for better results. We believe that this will provide for
better results than what is currently achievable with the published methods.

%-------------------------------------------------------------------------

\section{Technical Plan}

%-------------------------------------------------------------------------

\section{Experimental Plan}
We will build upon the open-source implementation of \cite{SRGAN} which uses
TensorFlow 1.2.

%-------------------------------------------------------------------------

\section{Sources of Data}
Image SR techniques are self-supervised in that explicit annotation of data is
unneeded. Given an image, a training pair can easily be generated by using the
downsampled image as the input to the SR network and the original image as the
ground truth. However, in order to integrate semantic segmentation knowledge,
we need to obtain the semantic layout for each image. There are two ways that we
plan on going about this. The first is to use datasets that already include
the segmentation such as the Cityscapes \cite{Cityscapes} dataset. The other
approach is to use existing segmentation networks like in
\cite{FullyConvolutionalSS} to generate the segmentation before the downsized
image is fed into our SR network. We plan to compare the results of the two to
see if the more accurate human annotated segmentations are required to get good
results from our technique, or if a fully-automated SR approach can be achieved.

To compare our results against those obtained from other papers, we plan on
using standard evaluation datasets that are used in \cite{SRGAN}. These include
Set5 \cite{Set5}, Set14 \cite{Set14}, and BSD100 which is the a subset of
BSD300 \cite{BSD300}

%-------------------------------------------------------------------------

\section{Partner Plan}
For the programming portion of the project, we plan to pair-program and
divide equally the implementation work. For experiments, we will individually
drive each one while working together as needed, with each person driving an
equal number of experiments. The report will be written collaboratively. The
person with more expertise on a given section will lead writing for that
section.

%-------------------------------------------------------------------------

\section{Speculation of Results}
Current state-of-the-art semantic segmentation methods already involve CNNs.
One may argue that a CNN based approach for SR would automatically learn
information regarding segmentation if it is useful for providing better
results. However, this argument does not take into account the difficulty of
learning such information. A parallel argument was made for residual networks
in \cite{ResNet} with regards to learning identity mappings. However, the
authors found that it was often difficult for networks to learn such mappings.
We predict that adding the semantic segmentation data explicitly to the network
will allow it to make more intelligent decisions along object boundaries. We
expect this will help provide more crisp edges. Because of the success with
using semantic masks at various resolutions in \cite{ImageSynthesis}, we expect
that our results will be better at high upsampling factors.

{\small
\bibliographystyle{ieee}
\bibliography{bib}
}

\end{document}
